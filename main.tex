\documentclass[12pt,a4paper]{amsart}

\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{macros}
\usepackage{natbib}

\usepackage{cleveref}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\title{PROVISIONAL: 2-Wasserstein on a graph?}

\author{Giovanni Pistone}
\address{de Castro Statistics, Collegio
  Carlo Alberto, piazza Vincenzo Arbarello 8, Torino IT}

\author{Luigi Montrucchio}
\address{Collegio
  Carlo Alberto, piazza Vincenzo Arbarello 8, Torino IT}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
  In the dynamical version of 2-Wasserstein, one considers the
  solutions of the continuity equation connecting two probability
  measures and optimizes on the vector fields. Wuchen Li claims to
  extend that to the finite state space but he does not succed. Here,
  we try to make it right.
\end{abstract}
\section{Gradient and divergence on a Graph}

We start with a review the well known formalism of
\href{https://en.wikipedia.org/wiki/Calculus_on_finite_weighted_graphs}
{Calculus on Finite Weighted Graphs}. See the Wiki and also
\citep{bollobas:1998}.

Let $G = (V,\omega)$ be a \emph{weighted simple connected graph}, were
$\omega$ is a zero-diagonal non-negative valued symmetric $V \times V$
matrix. The weight $\omega$ is associated to a distance in a way to be
discussed later.

The adjacency matrix $E = [(\omega(x,y) > 0)]$ splits (in many
ways) into the sum of two adjacency matrices of directed graphs on $V$
in such a way that $E = \overrightarrow E + \overleftarrow E$. Let
$L(G)$ be the vector space of $V \times V$ matrices whose support is
contained in the support of $E$. Similarly, we define the space of
symmetric matrices $S(G)$ and the space of skew-symmetric matrices
$A(G)$. Elements of $A(G)$ are also called \emph{vector fields} on
$G$. Both $S(G)$ and $A(G)$ are uniquely characterised by their
restriction on either $\overrightarrow E$ or $\overleftarrow E$.

For each given weight $\theta$ on $G$, that is, $\theta \in S(G)$ and
$\theta(x,y) > 0$ if $E(x,y)=1$, we define the \emph{inner product}
$\scalarat \theta \cdot \cdot$ for $a,b \in L(G)$ by
\begin{multline*}
  \scalarat \theta a b = \frac 12 \sum _ {x,y \in V} a(x,y)
  b(x,y)\theta(x,y) = \frac 12 \sum _ {E(x,y)=1} a(x,y)
  b(x,y)\theta(x,y) = \\ \sum_{\overrightarrow E(x,y)=1}
  \frac12(a(x,y)b(x,y)+a(y,x)b(y,x)) \theta(x,y)\ .
\end{multline*}

If $a$ and $b$ are one symmetric and the other skew-symmetric, then
$\scalarat \theta a b = 0$, irrespective of the weight $\theta$.
If $a$ and $b$ are either both symmetric, or both skew-symmetric, then
\begin{equation}\label{eq:inner0}
  \scalarat \theta a b = \sum_{\overrightarrow E(x,y)=1}
  a(x,y)b(x,y) \theta(x,y) \ .
\end{equation}

In this section, let us call \emph{potential} a mapping
$\phi \colon V \to \reals$, that is, $\phi\in L(V)$. We can linearly
map potentials into vector fields by the \emph{$G$-gradient} (or,
simply, \emph{gradient}) $\nabla \colon L(V) \to A(G)$, which is
defined by
\begin{equation}\label{eq:nabla}
  \nabla \phi(x,y) = \sqrt {\omega(x,y)} (\phi(x) - \phi(y))
\end{equation}
for all $x,y$ such that $E(x,y)=1$, and is zero on all other
couples. The meaning of the square rooth will be explained later.

As the graph $G$ is connected, it holds $\ker \nabla = \reals \one$,
that is, $\nabla\phi=0$ if, and only if, $\phi$ is constant.

\begin{example}[Graph distance] If $\omega(x,y) = d(x,y)^{-2}$ for a distance $d$ on
$V$, and $d$ is a graph distance, then
\begin{equation*}
  \nabla \phi (x,y)=\frac{\phi(x) - \phi(y)}{d(x,y)} \ .
\end{equation*}
Recall that, according to our definition, $\nabla \phi (x,y) = 0$ if
the edge $(x,y)$ does not belong to the graph. However, the constant
\begin{equation*}
  K(\phi) = \sup_{E(x,y)=1} \nabla \phi(x,y)
\end{equation*}
is indeed the Lipsczitz constant of $\phi$ because every couple $x,y$
is connected in the graph by a $d$-minimal path $x_0=x, \dots, x_n=y$
and
\begin{equation*}
  \phi(x) - \phi(y) = \sum_{i=1}^n \phi(x_{i-1}) - \phi(x_i) \leq
  \sum_{i=1}^n K(\phi) d(x_{i-1},x_i) = K(\phi) d(x,y) \ . 
\end{equation*}
\end{example}

Let $\rho$ be a positive weight on $V$ and use it to define an inner
product on potentials, that is, define $L^2(\rho)$. The gradient
operator maps this space to the space of vector fields $A(\theta)$
endowed with the inner product \eqref{eq:inner0},
$\nabla \colon L^2(\rho) \to A(\theta)$.  Let us compute the adjoint
$\nabla^* \colon A(\theta) \to L^2(\rho)$.

Let $b$ be any vector field. We have from \cref{eq:inner0,eq:nabla} that
\begin{multline*}
  \scalarat \theta {\nabla \phi}{b} = \sum_{\overrightarrow E(x,y)=1}
  \sqrt{\omega(x,y)}  (\phi(x) - \phi(y)) b(x,y) \theta(x,y) = \\
  \sum_{\overrightarrow E(x,y)=1} \sqrt{\omega(x,y)} \phi(x) b(x,y)
  \theta(x,y) - \sum_{\overrightarrow E(x,y)=1}
  \sqrt{\omega(x,y)}  \phi(y) b(x,y) \theta(x,y) = \\
  \sum_{\overrightarrow E(x,y)=1} \sqrt{\omega(x,y)} \phi(x) b(x,y)
  \theta(x,y) - \sum_{\overleftarrow E(x,y)=1}
  \sqrt{\omega(x,y)}  \phi(x) b(y,x) \theta(x,y) = \\
  \sum_{\overrightarrow E(x,y)=1} \sqrt{\omega(x,y)} \phi(x) b(x,y)
  \theta(x,y) + \sum_{\overleftarrow E(x,y)=1}
  \sqrt{\omega(x,y)}  \phi(x) b(x,y) \theta(x,y) = \\
  \sum_{E(x,y)=1} \sqrt{\omega(x,y)}  \phi(x) b(x,y) \theta(x,y) = 
  \sum_{x \in V} \phi(x) \sum_{y \in N(x)}
  \sqrt{\omega(x,y)}  b(x,y) \theta(x,y) = \\
  \sum_{x \in V} \left(\rho(x)^{-1} \sum_{y \in N(x)}
  \sqrt{\omega(x,y)}  b(x,y) \theta(x,y) \right) \phi(x) \rho(x)
\end{multline*}
where $N(x) = \setof{y}{E(x,y)=1}$ is the $G$-neighborhood of $x$. In
conclusion,
\begin{equation}\label{eq:nablastar}
  \nabla^* b(x) = \rho(x)^{-1} \sum_{y \in N(x)}
  \sqrt{\omega(x,y)}  b(x,y) \theta(x,y) \ .
\end{equation}

\begin{remark}
  Some authors call \emph{divergence} the operator defined on $A(G)$ by
\begin{equation*}
  \Div f = - \sum_{y \in N(x)} \sqrt{\omega(x,y)} f(x,y) \ .
\end{equation*}
In such a notation,
\begin{equation*}
  \nabla^* b = - \rho^{-1} \Div(\theta b) \ . 
\end{equation*}

In the same spirit, the following operator is called \emph{Laplacian}:
\begin{equation*}
  \Delta \phi (x) = \Div \nabla \phi (x)= - \sum_{y \in N(x)}
  \omega(x,y) (\phi(x)-\phi(y)) \ .
\end{equation*}
Notice that the weight $\omega$ is associated with the gradient
operator, while the weights $\rho$ and $\theta$ are associated with
the inner products. In specific applications of the formalim, some
special relation between the weitghs could hold.
\end{remark}

\begin{example}[Trees] Assume the graph is a tree and
  $\overrightarrow E$ is the rooted directed tree. In such a case,
  $N(x) = \set{x^+} \cup F(x)$, where $F$ is the floret of $x$, that
  is, $\overrightarrow E(x^+,x) = 1$ if $x$ is not the root and
  $\overrightarrow E(x,y)=1$ if, and only if, $y \in F(x)$. Every
  directed edge is of the form $(x^+,x)$ and the value of the gradient
  is $\sqrt{\omega(x^+,x)} (\phi(x^+) - \phi(x))$ for all non-root
  $x$. Given any vector field $b \in A(G)$, its potential is
  \begin{equation*}
    \phi(x) = C - \sum_{y \in \gamma(x)} b(y^+,y) \ ,
  \end{equation*}
where $\gamma(x)$ is the unique path from the root to $x$.
\end{example}

\begin{example}[Reversibility] An interesting case happens when $\theta$
can be extended to a probability function by adding diagonal values
$\theta(x,x)$ and $\rho$ equals the two margins,
\begin{equation*}
  \rho(z) = \sum_y \theta(z,y) = \sum_x \theta(x,z) \ .
\end{equation*}
In such a case the conditional probability functions are reversible,
\begin{equation*}
  \theta_{2|1}(y|x) \rho(x) = \theta_{1|2}(x|y) \rho(y) = \theta(x,y) \ ,
\end{equation*}
and there is a reversible random walk on the graph. Morover, the
operator $\nabla^*$ is a conditional expectation.
\end{example}

\section{Arens-Eells norms}

Recall that we work with a weighted graph $G = (V,\omega)$ and with
weights $\theta$ and $\rho$ respectively on the edge set
$\setof{\set{x,y}}{E(x,y)=1}$ and the vertex set $V$.

Let $L(V)$ the space of real functions on $V$ (potentials in the
previous section). Let $M_0(\rho)$ be the space of (density) functions
on $V$ such that $\sum_{z \in V} \xi(z) \rho(z) = 0$. Cf.
\citep{montrucchio|pistone:2019-arXiv:1905.07547}, where $\rho=1$. As
a vector space, $M_0(\rho)$ is a subspace for $L(V)$ of co-dimension
1. Every $\rho\xi$ has zero sum, hence it is and it is a linear
combination of differences of deltas, $(\delta_x-\delta_y)$,
$x,y \in V$. Using the connection of the graph and a change of sign in
the coefficients of the linear combination, we see that a generating
set is
\begin{equation*}
  \setof{\delta_x-\delta_y}{\overrightarrow E(x,y)=1} \ .
\end{equation*}

If $\phi \in L(V)$ and $\xi \in M_0(V)$, for some $a \in A(G)$ it
holds that
\begin{multline*}
  \rho(z)\xi(z) =
  \sum_{\overrightarrow E(x,y)=1} a(x,y) (\delta_x(z) - \delta_y(z)) = \\
  \sum_{\overrightarrow E(x,y)=1} a(x,y) (\delta_z(x) - \delta_z(y)) =
  \sum_{\overrightarrow E(x,y)=1} a(x,y) \omega(x,y)^{-1/2} \nabla
  \delta_z (x,y) \ ,
\end{multline*}
so that
\begin{multline*}
  \scalarat \rho \phi \xi = \scalarat{1}{\rho\phi}{\xi} = \\ \sum_{z \in V} \phi(z)
  \left(\sum_{\overrightarrow E(x,y)=1}
    a(x,y) \omega(x,y)^{-1/2} \nabla \delta_z (x,y)\right) = \\
  \sum_{\overrightarrow E(x,y)=1} \omega(x,y) ^{-1/2} a(x,y) \sum_{z
    \in V} \nabla \delta_z (x,y) \phi(z) = \\
  \sum_{\overrightarrow E(x,y)=1} \omega(x,y) ^{-1/2} a(x,y) \nabla
  \phi (x,y) = \\ \scalarat \theta
  {\omega^{-1/2}\theta^{-1} a} {\nabla \phi} = \scalarat
  \rho {\nabla^* (\omega^{-1/2} \theta^{-1} a)} {\phi} .
\end{multline*}
In fact, $\phi(u) = \sum_z \phi (z) \delta_z(u)$ and,
by linearity,
\begin{equation*}
 \nabla \phi (x,y) = \sum_z \phi(z)
\nabla \delta_z(x,y) \ .
\end{equation*}

It follows that $\rho \xi = \sum_{xy} a(x,y) (\delta_x-\delta_y)$ if, and
only if,
\begin{equation*}
  \xi(x) =  \nabla^* (\omega^{-1/2} \theta^{-1} a) (x) = \rho(x)^{-1} \sum_{y
    \in N(x)}  a(x,y) = - \rho(x)^{-1} \Div a (x) \ , 
\end{equation*}
where we have used \cref{eq:nablastar}. 
Equivalently, $\rho \xi + \Div a = 0$, and, in such a case,
\begin{equation*}
  \scalarat \rho \phi \xi = \scalarat {\theta} {a} {\omega^{-1/2} \theta^{-1} \nabla
    \phi} \ .
\end{equation*}

Now, consider the norm
\begin{multline*}
 \sup\setof{\scalarat \rho \phi \xi}{-1 \leq \omega^{-1/2} \theta^{-1} \nabla
    \phi \leq 1} = \\ \sup\setof{\scalarat \theta a
    {\omega^{-1/2} \theta^{-1} \nabla
    \phi }}{\rho \xi + \Div a = 0,
    -1 \leq \omega^{-1/2} \theta^{-1} \nabla
    \phi \leq 1} = \\
  \sup\setof{\sum_{\overrightarrow E(x,y) = 1} \avalof{a(x,y)} \theta(x,y)}{\rho \xi + \Div a = 0} \ ,
\end{multline*}

It is a generalisation of the \emph{Arens-Eells} norm of $\xi$ as
defined in \citep{montrucchio|pistone:2019-arXiv:1905.07547}.  In fact, if $\rho=1$, $\omega = d^{-2}$ and $\theta=d$, the sup is taken on the potentials
$\phi$ such that $\phi$ is in the unit $d$-Lipschitz ball and the value is $\sum \avalof{a(x,y)}d(x,y)$.

Other analogous norms could be considered, for example,
\begin{multline*}
  \sup\setof{\scalarat \rho \phi \xi}{\normat{L^2(\theta)}{\omega^{-1/2} \theta^{-1} \nabla \phi} \leq 1} = \\
    \sup\setof{\scalarat \theta a {\omega^{-1/2} \theta^{-1} \nabla\phi}}{\rho\xi + \Div a = 0,
  \normat{L^2(\theta)}{\omega^{-1/2} \theta^{-1} \nabla \phi} \leq 1} = \\
\sup\setof{\normof a _{L^2(\theta)}}{\rho \xi + \Div a = 0} \ .
\end{multline*}

This is a convex problem to be studied toghether with its dual. 

\section{Laplacian and Hodge decomposition}
\label{sec:laplacian}

We define the following symmetric form on potentials:
\begin{equation*}
  \Scalarat \theta \phi \psi = \scalarat \theta {\nabla\phi}
  {\nabla\psi} = \scalarat \rho  {A \phi} \psi \ , 
\end{equation*}
where $A \phi = \nabla^* \nabla\phi$, that is,
\begin{equation*}
  A\phi(x) = \sum_{y \in N(x)} \omega(x,y) (\phi(x) - \phi(y))
  \frac{\theta(x,y)}{\rho(x)} \ .
\end{equation*}
The operator $A$ is the
\emph{Laplacian}.

The symmetric form is separating for non-constant potentials,
\begin{equation*}
  0 = \Scalarat \theta \phi \phi = \sum_{xy \in E} \omega(x,y)
  (\phi(x) - \phi(y))^2 \theta(x,y) \ , 
\end{equation*}
that is, $\phi(x) = \phi(y)$ for all $x,y \in E$. It follows that the
symmetric form is a scalar product above on the space of sum-zero
potentials, that is, on the affine space of the probability simplex on
$V$. Same is true for each substace of codimension 1.

Let us study the image of the gradient operator in the space of vector
fields. The vector field $w$ belongs to the  orthogonal of the image of
$\nabla$ if, for all $\phi$,
\begin{equation*}
  0 = \scalarat \theta {\nabla \phi} w = \scalarat \rho \phi {\nabla^* w} \ ,
\end{equation*}
that is, if, and only if, $\nabla^* w  = 0$. The following
statement follow easily. It is sometimes called \emph{Hodge decomposition}.

\emph{Every vector field $v \in A(G)$ admits the $\theta$-orthogonal
  decomposition $v = \nabla \phi + w$, where $\phi$ is a potential
  defined up to a constant and $\nabla^* w = 0$.}

Clearly, the decomposition depends on $\theta$.

\section{Continuity equation}
\label{sec:continuity}

Let $\theta$ be a smooth mapping from the probability simplex on $V$ to the
cone of positive $S(G)$, $\theta \colon \rho \mapsto \theta(\rho) \in
S_+(G)$. Given a regular curve $[0,1] \ni t \mapsto \Phi(t)$ in the
space of zero-sum potentials, the ordinary differential equation
\begin{equation*}
  \derivby t \rho(t) +  \Div(\theta(\rho(t))\nabla \phi(t)) = 0 \ , \quad
  \rho(0) = \rho^0 \in \mathcal P_+(V) \ ,
\end{equation*}
has a local solution in $\mathcal P_+(V)$ because
\begin{equation*}
  \derivby t \sum_x \rho(x;t) = \scalarof {\dot \rho(t)}{\one} = 
 - \scalarof {\Div(\theta(\rho(t))\nabla \phi(t))}{1} = \scalarat
 {\theta(\rho)} {\nabla \Phi(t)}{\nabla 1}
 = 0 \ .
\end{equation*}

Given $\rho^0, \rho^1 \in \mathcal P_+(V)$, consider the value of the problem
\begin{align*}
 \inf &\int_0^1 \Scalarat {\theta(\rho(t))} {\phi(t)}{\phi(t)} \ dt
 \\
 &\derivby t \rho(t) +  \Div(\theta(\rho)\nabla \Phi(t)) = 0 \\
 &\rho(0) = \rho^0 \ , \quad \rho(1) = \rho^1.
\end{align*}

Let us write $L(\rho)\Phi = \Div(\theta(\rho)\nabla \Phi)$ and
consider that the conjugate of the quadratic form
$\Scalarat {\theta(\rho)} \phi \phi = \frac 12 \scalarof {L(\rho)\phi}
\psi$ is of the form
\begin{equation*}
  \frac 12 \scalarof {L(\rho)^\dagger \alpha} {\alpha} = \frac12 =
  \scalarof {L(\rho)^\dagger L(\rho) \phi} {L(\rho)(\phi)} = \frac12
  {L(\rho) \phi} \phi \ . 
\end{equation*}

Writing now $\Phi = L(\rho)^\dagger \dot \rho$ in the problem above,
we obtain the equivalent problem
\begin{align*}
 \inf &\int_0^1 \scalarof {L(\rho(t))^\dagger \dot \rho(t)}{\dot \rho(t)} \ dt
 \\
 &\rho(0) = \rho^0 \ , \quad \rho(1) = \rho^1 \ .
\end{align*}

In conclusion, the value of the problem is a squared Riemannian
distance for the metric whose tensor is $L(\rho)^\dagger$

\bibliographystyle{plain}
\bibliography{tutto}

\end{document}
