b\documentclass[12pt,a4paper]{amsart}

\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{macros}
\usepackage{natbib}

\usepackage{cleveref}

\title{2-Wasserstein on a graph}

\author{Giovanni Pistone} \address{de Castro Statistics, Collegio
  Carlo Alberto, piazza Vincenzo Arbarello 8, Torino IT}

\author{LM?}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
  In the dynamical version of 2-Wasserstein, one considers the
  solutions of the continuity equation connecting two probability
  measures and optimizes on the vector fields. Wuchen Li claims to
  extend that to the finite state space but he does not succed. Here,
  we try to make it right.
\end{abstract}
\section{Gradient and divergence on a Graph}

We start with a review the well known formalism of
\href{https://en.wikipedia.org/wiki/Calculus_on_finite_weighted_graphs}
{Calculus on Finite Weighted Graphs}. See the Wiki and also
\citep{bollobas:1998}.

Let $G = (V,\omega)$ be a \emph{weighted simple connected graph}, were
$\omega$ is a zero-diagonal non-negative symmetric $V \times V$
matrix. The weight $\omega$ is associated to a distance in a way to be
discussed later.

The adjacency matrix $E = [(\omega(x,y) > 0)]$ splits (in many
ways) into the sum of two adjacency matrices of directed graphs on $V$
in such a way that $E = \overrightarrow E + \overleftarrow E$. Let
$L(G)$ be the vector space of $V \times V$ matrices whose support is
contained in the support of $E$. Similarly, we define the space of
symmetric matrices $S(G)$ and the space of skew-symmetric matrices
$A(G)$. Elements of $A(G)$ are also called \emph{vector fields} on
$G$. Both $S(G)$ and $A(G)$ are uniquely characterised by their
restriction on either $\overrightarrow E$ or $\overleftarrow E$.

For each given weight $\theta$ on $G$, that is, $\theta \in S(G)$ and
$\theta(x,y) > 0$ if $E(x,y)=1$, we define the \emph{inner product}
$\scalarat \theta \cdot \cdot$ for $a,b \in L(G)$ by
\begin{multline*}
  \scalarat \theta a b = \frac 12 \sum _ {x,y \in V} a(x,y)
  b(x,y)\theta(x,y) = \frac 12 \sum _ {E(x,y)=1} a(x,y)
  b(x,y)\theta(x,y) = \\ \sum_{\overrightarrow E(x,y)=1}
  \frac12(a(x,y)b(x,y)+a(y,x)b(y,x)) \theta(x,y)\ .
\end{multline*}

If $a$ and $b$ are one symmetric and the other skew-symmetric, then
$\scalarat \theta a b = 0$, irrespective of the weight $\theta$.
If $a$ and $b$ are either both symmetric, or both skew-symmetric, then
\begin{equation}\label{eq:inner0}
  \scalarat \theta a b = \sum_{\overrightarrow E(x,y)=1}
  a(x,y)b(x,y) \theta(x,y) \ .
\end{equation}

In this section, let us call \emph{potential} a mapping
$\phi \colon V \to \reals$, that is, $\phi\in L(G)$. We can map
potentials into vector fields by the \emph{$G$-gradient} (or, simply,
\emph{gradient}) $\nabla \colon L(G) \to A(G)$, which is defined by
\begin{equation*}
  \nabla \phi(x,y) = \sqrt {\omega(x,y)} (\phi(x) - \phi(y)) \quad
  \text{for all} \quad x,y \in V \ .
\end{equation*}
The meaning of the square rooth will be explained later. As the graph
$G$ is connected, it holds $\ker \nabla = \reals \one$, that is,
$\nabla\phi=0$ if, and only if, $\phi$ is constant. If
$\omega(x,y) = d(x,y)^{-2}$ for a distance $d$ on $V$, then
\begin{equation*}
  \nabla \phi (x,y)=\frac{\phi(x) - \phi(y)}{d(x,y)} \ .
\end{equation*}

Let $\rho$ be a positive weight on $V$ and use it to define an inner
product on potentials, that is, consider $L^2(\rho)$. Le gradient
operator maps this space to the space of vector fields $A(G)$ endowed
with the inner product \eqref{eq:inner0}, $\nabla \colon L^2(\rho) \to A(G)$.
Let us compute the adjoint $\nabla^* \colon A(G) \to L^2(\rho)$.

Let $b$ be any vector field. We have from \cref{eq:inner0}
\begin{multline*}
  \scalarat \theta {\nabla \phi}{b} = \sum_{\overrightarrow E(x,y)=1}
  \sqrt{\omega(x,y)}  (\phi(x) - \phi(y)) b(x,y) \theta(x,y) = \\
  \sum_{\overrightarrow E(x,y)=1} \sqrt{\omega(x,y)} \phi(x) b(x,y)
  \theta(x,y) - \sum_{\overrightarrow E(x,y)=1}
  \sqrt{\omega(x,y)}  \phi(y) b(x,y) \theta(x,y) = \\
  \sum_{\overrightarrow E(x,y)=1} \sqrt{\omega(x,y)} \phi(x) b(x,y)
  \theta(x,y) - \sum_{\overleftarrow E(x,y)=1}
  \sqrt{\omega(x,y)}  \phi(x) b(y,x) \theta(x,y) = \\
  \sum_{\overrightarrow E(x,y)=1} \sqrt{\omega(x,y)} \phi(x) b(x,y)
  \theta(x,y) + \sum_{\overleftarrow E(x,y)=1}
  \sqrt{\omega(x,y)}  \phi(x) b(x,y) \theta(x,y) = \\
  \sum_{E(x,y)=1} \sqrt{\omega(x,y)}  \phi(x) b(x,y) \theta(x,y) = 
  \sum_{x \in V} \phi(x) \sum_{y \in N(x)}
  \sqrt{\omega(x,y)}  b(x,y) \theta(x,y) = \\
  \sum_{x \in V} \left(\sum_{y \in N(x)}
  \sqrt{\omega(x,y)}  b(x,y) \frac{\theta(x,y)}{\rho(x)} \right) \phi(x) \rho(x)
\end{multline*}
where $N(x) = \setof{y}{E(x,y)=1}$. In conclusion,
\begin{equation*}
  \nabla^* b(x) = \sum_{y \in N(x)}
  \sqrt{\omega(x,y)}  b(x,y) \frac{\theta(x,y)}{\rho(x)} \ .
\end{equation*}

Some Authors call \emph{divergence} the operator defined on $L(G)$ by
\begin{equation*}
  \Div f = - \sum_{y \in N(x)} f(x,y) \ .
\end{equation*}
In such a notation,
\begin{equation*}
  \nabla^* b = - \rho^{-1} \Div(\omega^{1/2} \theta f) \ . 
\end{equation*}


\subsection*{Trees} Assume the graph is a tree and $\overrightarrow
E$ is the directed tree with root. In such a case, $N(x) = \set{x^+}
\cup F(x)$, where $F$ is the floret of $x$, that is, $\overrightarrow E(x^+,x) =
1$ and $\overrightarrow E(x,y)=1$ if, and only if, $y \in F(x)$.

\subsection*{Reversibility} An interesting case happens when $\theta$
can be extended to a probability function by adding diagonal values
$\theta(x,x)$ and $\rho$ equals the two margins,
\begin{equation*}
  \rho(z) = \sum_y \theta(z,y) = \sum_x \theta(x,z) \ .
\end{equation*}
In such a case the conditional probability functions are reversible,
\begin{equation*}
  \theta_{2|1}(y|x) \rho(x) = \theta_{1|2}(x|y) \rho(y) = \theta(x,y) \ ,
\end{equation*}
and there is a reversible random walk on the graph.


\section{Arens-Eells norms}

Recall that we work with a weighted graph $G = (V,\omega)$ and we
define duality pairing with the weights $\theta$ and $\rho$
despectively on the edge set $\mathcal E$ and the vertex set $V$.

Let $L(V)$ the space of real functions on $V$ (potentials in the
previous section). Let $M_0(V)$ be the space of measures on $V$. As a
vector space, $M_0(V)$ is a subspace for $L(V)$ of co-dimension
1. Notice that $\phi \in L(V)$ and $\xi \in M_0(V)$ implies
\begin{multline*}
  \xi(z) =
\sum_{E(x,y)=1} a(x,y) (\delta_x(z) - \delta_y(z)) = \\
\sum_{E(x,y)=1} a(x,y) (\delta_z(x) - \delta_z(y)) = \sum_{E(x,y)=1}
a(x,y) \omega(x,y)^{-1/2} \nabla \delta_z (x,y) \ ,
\end{multline*}
and
\begin{multline*}
  \scalarat \rho \phi \xi = \sum_{z \in V} \phi(z)
  \left(\sum_{E(x,y)=1}
    a(x,y) \omega(x,y)^{-1/2} \nabla \delta_z (x,y)\right) \rho(z) = \\
  \sum_{E(x,y)=1} \omega(x,y) ^{-1/2} a(x,y) \sum_{z \in V} \nabla \delta_z
  (x,y) \phi(z) \rho(z) = \\ \sum_{E(x,y)=1} \omega(x,y) ^{-1/2} a(x,y)
  \nabla (\phi \cdot \rho) (x,y) = \\ \scalarat \theta
  {\omega^{-1/2}\theta^{-1} a} {\nabla (\phi\cdot\rho)} =
  \scalarat \rho {\rho \nabla^* (\omega^{-1/2} \theta^{-1} a)} {\phi} .
\end{multline*}

It follows that
\begin{equation*}
  \scalarat \rho \phi \xi = \scalarat {\omega^{-1/2}} {a} {\nabla (\phi\cdot\rho)}
\end{equation*}
and
\begin{equation*}
  \xi(x) = \rho(x) \nabla^* (\omega^{-1/2} \theta^{-1} a) (x) = \sum_{y
    \in N(x)}  a(x,y) = - \Div a \ . 
\end{equation*}

Now, the \emph{Arens-Eells} norm of $\xi$ is
\begin{multline*}
  \sup\setof{\scalarat \rho \phi \xi}{\xi + \Div a = 0,
  -1 \leq \omega^{-1/2} \nabla(\phi\cdot\rho) \leq 1} = \\ \sup\setof{\sum
  \avalof{a(x,y)}}{\xi + \Div a = 0} \ ,
\end{multline*}
where all the summations are taken on $\set{E(x,y)=1}$.

In particular, if $\omega = d^2$, the supremum is taken on the
potentials $\phi$ such that $\phi\cdot\rho$ is in the unit
$d$-Lipschitz ball.

Other analogous norms could be considered, for example,
\begin{equation*}
  \sup\setof{\scalarat \rho \phi \xi}{\xi + \Div a = 0,
  \normof{\omega^{-1/2} \nabla(\phi\cdot\rho)}_2 \leq 1} = 
\sup\setof{\normof a _2}{\xi + \Div a = 0} \ .
\end{equation*}

This is a convex problem to be studied. 

\section{Laplacian and Hodge decomposition}
\label{sec:laplacian}

We define the following symmetric form on potentials:
\begin{equation*}
  \Scalarat \theta \phi \psi = \scalarat \theta {\nabla\phi}
  {\nabla\psi} = \scalarat \rho  {A \phi} \psi \ , 
\end{equation*}
where $A \phi = \nabla^* \nabla\phi$, that is,
\begin{equation*}
  A\phi(x) = \sum_{y \in N(x)} \omega(x,y) (\phi(x) - \phi(y))
  \frac{\theta(x,y)}{\rho(x)} \ .
\end{equation*}
The operator $A$ is the
\emph{Laplacian}.

The symmetric form is separating for non-constant potentials,
\begin{equation*}
  0 = \Scalarat \theta \phi \phi = \sum_{xy \in E} \omega(x,y)
  (\phi(x) - \phi(y))^2 \theta(x,y) \ , 
\end{equation*}
that is, $\phi(x) = \phi(y)$ for all $x,y \in E$. It follows that the
symmetric form is a scalar product above on the space of sum-zero
potentials, that is, on the affine space of the probability simplex on
$V$. Same is true for each substace of codimension 1.

Let us study the image of the gradient operator in the space of vector
fields. The vector field $w$ belongs to the  orthogonal of the image of
$\nabla$ if, for all $\phi$,
\begin{equation*}
  0 = \scalarat \theta {\nabla \phi} w = \scalarat \rho \phi {\nabla^* w} \ ,
\end{equation*}
that is, if, and only if, $\nabla^* w  = 0$. The following
statement follow easily. It is sometimes called \emph{Hodge decomposition}.

\emph{Every vector field $v \in A(G)$ admits the $\theta$-orthogonal
  decomposition $v = \nabla \phi + w$, where $\phi$ is a potential
  defined up to a constant and $\nabla^* w = 0$.}

Clearly, the decomposition depends on $\theta$.

\section{Continuity equation}
\label{sec:continuity}

Let $\theta$ be a smooth mapping from the probability simplex on $V$ to the
cone of positive $S(G)$, $\theta \colon \rho \mapsto \theta(\rho) \in
S_+(G)$. Given a regular curve $[0,1] \ni t \mapsto \Phi(t)$ in the
space of zero-sum potentials, the ordinary differential equation
\begin{equation*}
  \derivby t \rho(t) +  \Div(\theta(\rho(t))\nabla \phi(t)) = 0 \ , \quad
  \rho(0) = \rho^0 \in \mathcal P_+(V) \ ,
\end{equation*}
has a local solution in $\mathcal P_+(V)$ because
\begin{equation*}
  \derivby t \sum_x \rho(x;t) = \scalarof {\dot \rho(t)}{\one} = 
 - \scalarof {\Div(\theta(\rho(t))\nabla \phi(t))}{1} = \scalarat
 {\theta(\rho)} {\nabla \Phi(t)}{\nabla 1}
 = 0 \ .
\end{equation*}

Given $\rho^0, \rho^1 \in \mathcal P_+(V)$, consider the value of the problem
\begin{align*}
 \inf &\int_0^1 \Scalarat {\theta(\rho(t))} {\phi(t)}{\phi(t)} \ dt
 \\
 &\derivby t \rho(t) +  \Div(\theta(\rho)\nabla \Phi(t)) = 0 \\
 &\rho(0) = \rho^0 \ , \quad \rho(1) = \rho^1.
\end{align*}

Let us write $L(\rho)\Phi = \Div(\theta(\rho)\nabla \Phi)$ and
consider that the conjugate of the quadratic form
$\Scalarat {\theta(\rho)} \phi \phi = \frac 12 \scalarof {L(\rho)\phi}
\psi$ is of the form
\begin{equation*}
  \frac 12 \scalarof {L(\rho)^\dagger \alpha} {\alpha} = \frac12 =
  \scalarof {L(\rho)^\dagger L(\rho) \phi} {L(\rho)(\phi)} = \frac12
  {L(\rho) \phi} \phi \ . 
\end{equation*}

Writing now $\Phi = L(\rho)^\dagger \dot \rho$ in the problem above,
we obtain the equivalent problem
\begin{align*}
 \inf &\int_0^1 \scalarof {L(\rho(t))^\dagger \dot \rho(t)}{\dot \rho(t)} \ dt
 \\
 &\rho(0) = \rho^0 \ , \quad \rho(1) = \rho^1 \ .
\end{align*}

In conclusion, the value of the problem is a squared Riemannian
distance for the metric whose tensor is $L(\rho)^\dagger$

\bibliographystyle{plain}
\bibliography{tutto}

\end{document}
